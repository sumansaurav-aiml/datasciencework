# Integration Tests
from src.main import RecSys
import pytest
from unittest import mock
import os
import pandas as pd


@pytest.fixture(scope='module')
def folder_path():
    # all test csv files are under tests folder
    folder_path = 'tests'
    yield folder_path


@pytest.fixture(scope='module')
def job_10(folder_path):
    jobid = 10
    # Clearing the prediction files and logs before starting test
    rootdir = './' + folder_path + '/data/jobid' + str(jobid) + '/predictionfiles'
    for subdir, dirs, files in os.walk(rootdir):
        for filename in files:
            if filename != 'TEST_DATA_EVALUATED_SCORE.CSV':
                os.remove(os.path.join(rootdir, filename))
    yield jobid


@mock.patch('src.main.RecSys._mark_exception_in_database_for_job_execution')
@mock.patch('src.main.RecSys._mark_job_completed_in_database')
@mock.patch('src.main.RecSys._run_all_export')
@mock.patch('src.main.RecSys._run_all_imports')
@mock.patch('src.main.RecSys._create_folders_to_save_csv')
@mock.patch('src.main.RecSys._execute_job_start_script')
def test_run_main_job(mock__execute_job_start_script, mock__create_folders_to_save_csv, mock__run_all_imports,
                      mock__run_all_export, mock__mark_job_completed_in_database,
                      mock__mark_exception_in_database_for_job_execution, job_10, folder_path):
    with pytest.raises(SystemExit) as e:
        os.environ['APP_DBPROVIDER'] = 'dummy_db_provider'
        os.environ['APP_USER'] = 'dummy_user'
        os.environ['APP_PASSWORD'] = 'dummy_pass'
        os.environ['APP_CONNECTIONSTRING'] = 'dummy_conn_string'
        mock__execute_job_start_script.return_value = job_10
        recmain = RecSys(20,
                         3,
                         True)
        recmain.run_main_job()
    assert "0" in str(e.value)

    if os.path.isfile('./' + folder_path + '/data/jobid' + str(
            job_10) + '/predictionfiles/TOP_COL_FILTER_REC_PROD_FOR_PORTALS_USERS.CSV'):
        assert True
    else:
        assert False

    if os.path.isfile(
            './' + folder_path + '/data/jobid' + str(job_10) + '/predictionfiles/TOP_HYBRID_REC_FOR_PORTALS_USERS.CSV'):
        assert True
    else:
        assert False

    if os.path.isfile('./' + folder_path + '/data/jobid' + str(
            job_10) + '/predictionfiles/TOP_OWN_PAST_RATED_PROD_FOR_PORTALS_USERS.CSV'):
        assert True
    else:
        assert False

    if os.path.isfile(
            './' + folder_path + '/data/jobid' + str(job_10) + '/predictionfiles/TOP_TRENDING_PROD_FOR_PORTALS.CSV'):
        assert True
    else:
        assert False

    if os.path.isfile('./' + folder_path + '/data/jobid' + str(job_10) + '/predictionfiles/top_n_neighbours.CSV'):
        assert True
    else:
        assert False


def test_run_main_job_metric_compare(job_10, folder_path):
    # USER_ACTIVITY_WITH_TIME_PORTAL_TEST has a test activity data from year 2021, this will be our relevant item data
    df = pd.read_csv(
        './' + folder_path + '/data/jobid' + str(job_10) + '/trainingfiles/USER_ACTIVITY_WITH_TIME_PORTAL_TEST.CSV')
    # this file got generated by the model
    df_rec = pd.read_csv(
        './' + folder_path + '/data/jobid' + str(job_10) + '/predictionfiles/TOP_HYBRID_REC_FOR_PORTALS_USERS.CSV')
    '''this file has the standard recall and precision value which was recorded on a training dataset
        Same dataset has been used to generate TOP_HYBRID_REC_FOR_PORTALS_USERS.
        Under ideal case, there should not be a drop in the precision and recall and if so happens then 
        the new model will not pass our Integration test.'''
    df_metric = pd.read_csv(
        './' + folder_path + '/data/jobid' + str(job_10) + '/predictionfiles/TEST_DATA_EVALUATED_SCORE.CSV')

    # calculate precision and recall at k
    def precision(actual, predicted, top_n):
        act_set = set(actual)
        pred_set = set(predicted[:top_n])
        result = len(act_set & pred_set) / float(top_n)
        return result

    # tp/tp+fn - total actual +ve
    def recall(actual, predicted, top_n):
        act_set = set(actual)
        pred_set = set(predicted[:top_n])
        result = len(act_set & pred_set) / float(len(act_set))
        return result

    columns = ["USER_ID", "PORTAL", "PRECISION@5", "RECALL@5", "PRECISION@10", "RECALL@10", "PRECISION@20", "RECALL@20"]
    final_analysis_df = pd.DataFrame(columns=columns)

    for user in df.USER_ID.unique().tolist():
        if df_rec[df_rec['USER_ID'] == user].shape[0] > 0:
            portal = df[df['USER_ID'] == user][:1].iat[0, 4]
            relprod = df[df['USER_ID'] == user].ITEM_ID.unique().tolist()
            recprod = df_rec[df_rec['USER_ID'] == user].ITEM_ID.tolist()

            new_row = {'USER_ID': user,
                       'PORTAL': portal,
                       'PRECISION@5': precision(relprod, recprod, top_n=5),
                       'RECALL@5': recall(relprod, recprod, top_n=5),
                       'PRECISION@10': precision(relprod, recprod, top_n=10),
                       'RECALL@10': recall(relprod, recprod, top_n=10),
                       'PRECISION@20': precision(relprod, recprod, top_n=20),
                       'RECALL@20': recall(relprod, recprod, top_n=20)}
            final_analysis_df = final_analysis_df.append(new_row, ignore_index=True)
    final_analysis_df = final_analysis_df[
        ["PORTAL", "PRECISION@5", "RECALL@5", "PRECISION@10", "RECALL@10", "PRECISION@20", "RECALL@20"]].groupby(
        ["PORTAL"], as_index=False).agg(
        {"PRECISION@5": 'mean', "RECALL@5": 'mean', "PRECISION@10": 'mean', "RECALL@10": 'mean', "PRECISION@20": 'mean',
         "RECALL@20": 'mean'}).reset_index()

    final_analysis_df.to_csv('./' + folder_path + '/data/jobid' + str(
        job_10) + '/predictionfiles/TEST_DATA_EVALUATED_SCORE_FROM_INTEGRATION.CSV', index=False)
    for p in df_metric.PORTAL.unique().tolist():
        if final_analysis_df[final_analysis_df['PORTAL'] == p].shape[0] > 0:
            # PRECISION@5
            if round(final_analysis_df[final_analysis_df['PORTAL'] == p].iat[0, 2], 2) < round(
                    df_metric[df_metric['PORTAL'] == p].iat[
                        0, 2], 2):
                assert False
            # RECALL@5
            if round(final_analysis_df[final_analysis_df['PORTAL'] == p].iat[0, 3], 2) < round(
                    df_metric[df_metric['PORTAL'] == p].iat[
                        0, 3], 2):
                assert False
            # PRECISION@10
            if round(final_analysis_df[final_analysis_df['PORTAL'] == p].iat[0, 4], 2) < round(
                    df_metric[df_metric['PORTAL'] == p].iat[
                        0, 4], 2):
                assert False
            # RECALL@10
            if round(final_analysis_df[final_analysis_df['PORTAL'] == p].iat[0, 5], 2) < round(
                    df_metric[df_metric['PORTAL'] == p].iat[
                        0, 5], 2):
                assert False
            # PRECISION@20
            if round(final_analysis_df[final_analysis_df['PORTAL'] == p].iat[0, 6], 2) < round(
                    df_metric[df_metric['PORTAL'] == p].iat[
                        0, 6], 2):
                assert False
            # RECALL@20
            if round(final_analysis_df[final_analysis_df['PORTAL'] == p].iat[0, 7], 2) < round(
                    df_metric[df_metric['PORTAL'] == p].iat[
                        0, 7], 2):
                assert False
