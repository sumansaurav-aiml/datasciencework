{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNDL_R7_Project1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUBHdHaZo0j4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2d69a54f-43ac-4647-c72a-b7a4228ad2e4"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import numpy as np\n",
        "import h5py as h5\n",
        "import os\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn import metrics"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGeCI9Q6pspy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "a77c00f1-c08f-48e8-f9f0-adc22a7777ee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/4AHYJ8b0KKee3JbQxx3TodYXhXH6Cjr-h6t4S4GUmy-iHx0yeusRhgI\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6e54dh1ptmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4d547dc2-b862-42ab-d625-fa471f197bab"
      },
      "source": [
        "path = '/content/drive/My Drive/ColabNotebooks/NNDL/'\n",
        "os.chdir(os.path.dirname(os.path.abspath(path)))\n",
        "\n",
        "print(os.getcwd())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ColabNotebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC8MfTuPpFUp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "1a22596c-240e-4364-e1df-939dc2fb8420"
      },
      "source": [
        "f = h5.File(\"NNDL/SVHN_single_grey1-2.h5\", \"r\")\n",
        "# Get and print list of datasets within the H5 file\n",
        "datasetNames = [n for n in f.keys()]\n",
        "for n in datasetNames:\n",
        "  print(n)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_test\n",
            "X_train\n",
            "X_val\n",
            "y_test\n",
            "y_train\n",
            "y_val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV8WHJo-wY0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = f['X_test']\n",
        "X_train = f['X_train']\n",
        "X_val = f['X_val']\n",
        "y_test = f['y_test']\n",
        "y_train = f['y_train']\n",
        "y_val = f['y_val']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q9_HPOAwwcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b32c4205-66c4-4ecc-9033-a060fd57fac5"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42000, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92Lu43es6tzF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "65386306-b605-415e-e510-af906a79fda2"
      },
      "source": [
        "#vizualizing one of the image\n",
        "imgplot = plt.imshow(X_train[3])\n",
        "print('number is: ',y_train[3])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number is:  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWkuBMuGB6DJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "08b96d97-da2c-4d3d-fc9d-01d1b56d18bc"
      },
      "source": [
        "#hot encoding\n",
        "number_of_class = len(np.unique(y_train))\n",
        "print(number_of_class)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=number_of_class)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=number_of_class)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=number_of_class)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDRlmuu7KNFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "46a608c1-b9bc-4018-b0a1-fb3d7c1b5864"
      },
      "source": [
        "y_train[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpF4qDstKNcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Reshape data from 2D to 1D -> 32x32 to 1024\n",
        "model.add(tf.keras.layers.Reshape((1024,),input_shape=(32,32,)))\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dropout(0.1))\n",
        "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
        "\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "# Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(number_of_class, activation='softmax'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz0qprIna3Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFWOVRbSa6-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "3736834d-fc59-401f-e587-e57be517ebc2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               205000    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 241,906\n",
            "Trainable params: 239,058\n",
            "Non-trainable params: 2,848\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf7jFJQta_wR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29a5bfa0-a9e0-43d8-cf65-33fb50f32c23"
      },
      "source": [
        "model.fit(np.array(X_train), np.array(y_train), validation_data=(np.array(X_test), np.array(y_test)), epochs = 200, batch_size = 64) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 2.4374 - accuracy: 0.2224 - val_loss: 1.4974 - val_accuracy: 0.5390\n",
            "Epoch 2/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.6915 - accuracy: 0.4242 - val_loss: 1.2437 - val_accuracy: 0.6257\n",
            "Epoch 3/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.4614 - accuracy: 0.5121 - val_loss: 1.0765 - val_accuracy: 0.6813\n",
            "Epoch 4/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.3384 - accuracy: 0.5628 - val_loss: 1.0272 - val_accuracy: 0.6880\n",
            "Epoch 5/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.2511 - accuracy: 0.5966 - val_loss: 0.9448 - val_accuracy: 0.7166\n",
            "Epoch 6/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.1892 - accuracy: 0.6205 - val_loss: 0.9773 - val_accuracy: 0.6996\n",
            "Epoch 7/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.1299 - accuracy: 0.6410 - val_loss: 0.8149 - val_accuracy: 0.7567\n",
            "Epoch 8/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.0739 - accuracy: 0.6605 - val_loss: 0.9770 - val_accuracy: 0.6991\n",
            "Epoch 9/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.0442 - accuracy: 0.6678 - val_loss: 0.8094 - val_accuracy: 0.7499\n",
            "Epoch 10/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 1.0058 - accuracy: 0.6815 - val_loss: 0.7781 - val_accuracy: 0.7611\n",
            "Epoch 11/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.9749 - accuracy: 0.6928 - val_loss: 0.7793 - val_accuracy: 0.7601\n",
            "Epoch 12/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.9502 - accuracy: 0.7017 - val_loss: 0.7630 - val_accuracy: 0.7642\n",
            "Epoch 13/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.9378 - accuracy: 0.7045 - val_loss: 0.6809 - val_accuracy: 0.7953\n",
            "Epoch 14/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.9114 - accuracy: 0.7144 - val_loss: 0.8432 - val_accuracy: 0.7409\n",
            "Epoch 15/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.9012 - accuracy: 0.7197 - val_loss: 0.7192 - val_accuracy: 0.7778\n",
            "Epoch 16/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.8780 - accuracy: 0.7270 - val_loss: 0.7629 - val_accuracy: 0.7614\n",
            "Epoch 17/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.8644 - accuracy: 0.7288 - val_loss: 0.6609 - val_accuracy: 0.7987\n",
            "Epoch 18/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.8518 - accuracy: 0.7329 - val_loss: 0.6217 - val_accuracy: 0.8123\n",
            "Epoch 19/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.8351 - accuracy: 0.7401 - val_loss: 0.6266 - val_accuracy: 0.8076\n",
            "Epoch 20/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.8276 - accuracy: 0.7416 - val_loss: 0.6616 - val_accuracy: 0.7923\n",
            "Epoch 21/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.8057 - accuracy: 0.7493 - val_loss: 0.6957 - val_accuracy: 0.7819\n",
            "Epoch 22/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7969 - accuracy: 0.7507 - val_loss: 0.5885 - val_accuracy: 0.8216\n",
            "Epoch 23/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7888 - accuracy: 0.7538 - val_loss: 0.6544 - val_accuracy: 0.7986\n",
            "Epoch 24/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7779 - accuracy: 0.7591 - val_loss: 0.5918 - val_accuracy: 0.8210\n",
            "Epoch 25/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7705 - accuracy: 0.7610 - val_loss: 0.6076 - val_accuracy: 0.8159\n",
            "Epoch 26/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7581 - accuracy: 0.7653 - val_loss: 0.6549 - val_accuracy: 0.8001\n",
            "Epoch 27/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7488 - accuracy: 0.7666 - val_loss: 0.5834 - val_accuracy: 0.8230\n",
            "Epoch 28/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7479 - accuracy: 0.7675 - val_loss: 0.7604 - val_accuracy: 0.7689\n",
            "Epoch 29/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7463 - accuracy: 0.7689 - val_loss: 0.5765 - val_accuracy: 0.8266\n",
            "Epoch 30/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7305 - accuracy: 0.7738 - val_loss: 0.7658 - val_accuracy: 0.7762\n",
            "Epoch 31/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7270 - accuracy: 0.7735 - val_loss: 0.6146 - val_accuracy: 0.8101\n",
            "Epoch 32/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7163 - accuracy: 0.7764 - val_loss: 0.6231 - val_accuracy: 0.8067\n",
            "Epoch 33/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7098 - accuracy: 0.7792 - val_loss: 0.7419 - val_accuracy: 0.7788\n",
            "Epoch 34/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.7028 - accuracy: 0.7824 - val_loss: 0.5369 - val_accuracy: 0.8372\n",
            "Epoch 35/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6955 - accuracy: 0.7850 - val_loss: 0.5280 - val_accuracy: 0.8414\n",
            "Epoch 36/200\n",
            "657/657 [==============================] - 6s 9ms/step - loss: 0.6886 - accuracy: 0.7850 - val_loss: 0.5475 - val_accuracy: 0.8320\n",
            "Epoch 37/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6857 - accuracy: 0.7889 - val_loss: 0.5723 - val_accuracy: 0.8258\n",
            "Epoch 38/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6784 - accuracy: 0.7895 - val_loss: 0.5410 - val_accuracy: 0.8337\n",
            "Epoch 39/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6751 - accuracy: 0.7898 - val_loss: 0.7400 - val_accuracy: 0.7717\n",
            "Epoch 40/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6764 - accuracy: 0.7892 - val_loss: 0.6187 - val_accuracy: 0.8089\n",
            "Epoch 41/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6720 - accuracy: 0.7933 - val_loss: 0.6170 - val_accuracy: 0.8093\n",
            "Epoch 42/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6559 - accuracy: 0.7971 - val_loss: 0.5373 - val_accuracy: 0.8368\n",
            "Epoch 43/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6518 - accuracy: 0.7979 - val_loss: 0.5487 - val_accuracy: 0.8342\n",
            "Epoch 44/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6517 - accuracy: 0.7988 - val_loss: 0.6064 - val_accuracy: 0.8121\n",
            "Epoch 45/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6533 - accuracy: 0.7960 - val_loss: 0.5504 - val_accuracy: 0.8324\n",
            "Epoch 46/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6445 - accuracy: 0.8023 - val_loss: 0.5094 - val_accuracy: 0.8454\n",
            "Epoch 47/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6350 - accuracy: 0.8045 - val_loss: 0.5097 - val_accuracy: 0.8449\n",
            "Epoch 48/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6334 - accuracy: 0.8020 - val_loss: 0.5079 - val_accuracy: 0.8439\n",
            "Epoch 49/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6276 - accuracy: 0.8034 - val_loss: 0.5724 - val_accuracy: 0.8247\n",
            "Epoch 50/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6275 - accuracy: 0.8047 - val_loss: 0.5479 - val_accuracy: 0.8336\n",
            "Epoch 51/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6199 - accuracy: 0.8068 - val_loss: 0.5542 - val_accuracy: 0.8305\n",
            "Epoch 52/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6195 - accuracy: 0.8074 - val_loss: 0.5146 - val_accuracy: 0.8431\n",
            "Epoch 53/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6154 - accuracy: 0.8094 - val_loss: 0.4958 - val_accuracy: 0.8521\n",
            "Epoch 54/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6116 - accuracy: 0.8094 - val_loss: 0.5130 - val_accuracy: 0.8407\n",
            "Epoch 55/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6061 - accuracy: 0.8111 - val_loss: 0.5109 - val_accuracy: 0.8451\n",
            "Epoch 56/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6098 - accuracy: 0.8094 - val_loss: 0.6403 - val_accuracy: 0.8013\n",
            "Epoch 57/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6051 - accuracy: 0.8109 - val_loss: 0.5070 - val_accuracy: 0.8453\n",
            "Epoch 58/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5928 - accuracy: 0.8166 - val_loss: 0.5025 - val_accuracy: 0.8468\n",
            "Epoch 59/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.6000 - accuracy: 0.8143 - val_loss: 0.4916 - val_accuracy: 0.8522\n",
            "Epoch 60/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5874 - accuracy: 0.8159 - val_loss: 0.4861 - val_accuracy: 0.8524\n",
            "Epoch 61/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5891 - accuracy: 0.8173 - val_loss: 0.5687 - val_accuracy: 0.8262\n",
            "Epoch 62/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5907 - accuracy: 0.8167 - val_loss: 0.5600 - val_accuracy: 0.8279\n",
            "Epoch 63/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5883 - accuracy: 0.8176 - val_loss: 0.5019 - val_accuracy: 0.8477\n",
            "Epoch 64/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5829 - accuracy: 0.8185 - val_loss: 0.5029 - val_accuracy: 0.8447\n",
            "Epoch 65/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5848 - accuracy: 0.8172 - val_loss: 0.5430 - val_accuracy: 0.8332\n",
            "Epoch 66/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5806 - accuracy: 0.8203 - val_loss: 0.5315 - val_accuracy: 0.8357\n",
            "Epoch 67/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5729 - accuracy: 0.8224 - val_loss: 0.4739 - val_accuracy: 0.8562\n",
            "Epoch 68/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5604 - accuracy: 0.8253 - val_loss: 0.5185 - val_accuracy: 0.8403\n",
            "Epoch 69/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5718 - accuracy: 0.8212 - val_loss: 0.5204 - val_accuracy: 0.8395\n",
            "Epoch 70/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5752 - accuracy: 0.8223 - val_loss: 0.5211 - val_accuracy: 0.8426\n",
            "Epoch 71/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5676 - accuracy: 0.8235 - val_loss: 0.5012 - val_accuracy: 0.8473\n",
            "Epoch 72/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5653 - accuracy: 0.8259 - val_loss: 0.5438 - val_accuracy: 0.8352\n",
            "Epoch 73/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5625 - accuracy: 0.8280 - val_loss: 0.4891 - val_accuracy: 0.8509\n",
            "Epoch 74/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5542 - accuracy: 0.8279 - val_loss: 0.4683 - val_accuracy: 0.8588\n",
            "Epoch 75/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5560 - accuracy: 0.8270 - val_loss: 0.4700 - val_accuracy: 0.8571\n",
            "Epoch 76/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5552 - accuracy: 0.8285 - val_loss: 0.4934 - val_accuracy: 0.8513\n",
            "Epoch 77/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5491 - accuracy: 0.8290 - val_loss: 0.4552 - val_accuracy: 0.8627\n",
            "Epoch 78/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5578 - accuracy: 0.8275 - val_loss: 0.4814 - val_accuracy: 0.8547\n",
            "Epoch 79/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5477 - accuracy: 0.8296 - val_loss: 0.4982 - val_accuracy: 0.8482\n",
            "Epoch 80/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5446 - accuracy: 0.8297 - val_loss: 0.4757 - val_accuracy: 0.8551\n",
            "Epoch 81/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5466 - accuracy: 0.8312 - val_loss: 0.4495 - val_accuracy: 0.8636\n",
            "Epoch 82/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5417 - accuracy: 0.8309 - val_loss: 0.4561 - val_accuracy: 0.8626\n",
            "Epoch 83/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5414 - accuracy: 0.8322 - val_loss: 0.4851 - val_accuracy: 0.8517\n",
            "Epoch 84/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5401 - accuracy: 0.8323 - val_loss: 0.4545 - val_accuracy: 0.8628\n",
            "Epoch 85/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5327 - accuracy: 0.8360 - val_loss: 0.4483 - val_accuracy: 0.8649\n",
            "Epoch 86/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5345 - accuracy: 0.8328 - val_loss: 0.4471 - val_accuracy: 0.8648\n",
            "Epoch 87/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5353 - accuracy: 0.8322 - val_loss: 0.4994 - val_accuracy: 0.8479\n",
            "Epoch 88/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5302 - accuracy: 0.8350 - val_loss: 0.4517 - val_accuracy: 0.8635\n",
            "Epoch 89/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5318 - accuracy: 0.8346 - val_loss: 0.4697 - val_accuracy: 0.8569\n",
            "Epoch 90/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5306 - accuracy: 0.8349 - val_loss: 0.4805 - val_accuracy: 0.8525\n",
            "Epoch 91/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5263 - accuracy: 0.8364 - val_loss: 0.4909 - val_accuracy: 0.8532\n",
            "Epoch 92/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5282 - accuracy: 0.8344 - val_loss: 0.4856 - val_accuracy: 0.8545\n",
            "Epoch 93/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5228 - accuracy: 0.8380 - val_loss: 0.4717 - val_accuracy: 0.8547\n",
            "Epoch 94/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5192 - accuracy: 0.8396 - val_loss: 0.4804 - val_accuracy: 0.8533\n",
            "Epoch 95/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5145 - accuracy: 0.8406 - val_loss: 0.4563 - val_accuracy: 0.8619\n",
            "Epoch 96/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5170 - accuracy: 0.8399 - val_loss: 0.5455 - val_accuracy: 0.8322\n",
            "Epoch 97/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5124 - accuracy: 0.8400 - val_loss: 0.4645 - val_accuracy: 0.8602\n",
            "Epoch 98/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5209 - accuracy: 0.8371 - val_loss: 0.4894 - val_accuracy: 0.8501\n",
            "Epoch 99/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5197 - accuracy: 0.8383 - val_loss: 0.4365 - val_accuracy: 0.8686\n",
            "Epoch 100/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5098 - accuracy: 0.8409 - val_loss: 0.4747 - val_accuracy: 0.8542\n",
            "Epoch 101/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5144 - accuracy: 0.8390 - val_loss: 0.4339 - val_accuracy: 0.8686\n",
            "Epoch 102/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5047 - accuracy: 0.8430 - val_loss: 0.4374 - val_accuracy: 0.8674\n",
            "Epoch 103/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5075 - accuracy: 0.8425 - val_loss: 0.4804 - val_accuracy: 0.8526\n",
            "Epoch 104/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5020 - accuracy: 0.8435 - val_loss: 0.4744 - val_accuracy: 0.8577\n",
            "Epoch 105/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5029 - accuracy: 0.8456 - val_loss: 0.4789 - val_accuracy: 0.8566\n",
            "Epoch 106/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5058 - accuracy: 0.8412 - val_loss: 0.5307 - val_accuracy: 0.8359\n",
            "Epoch 107/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4974 - accuracy: 0.8456 - val_loss: 0.4926 - val_accuracy: 0.8481\n",
            "Epoch 108/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4921 - accuracy: 0.8453 - val_loss: 0.4479 - val_accuracy: 0.8667\n",
            "Epoch 109/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.5031 - accuracy: 0.8439 - val_loss: 0.7065 - val_accuracy: 0.7933\n",
            "Epoch 110/200\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.5047 - accuracy: 0.8425 - val_loss: 0.4318 - val_accuracy: 0.8696\n",
            "Epoch 111/200\n",
            "657/657 [==============================] - 6s 9ms/step - loss: 0.4942 - accuracy: 0.8456 - val_loss: 0.4291 - val_accuracy: 0.8687\n",
            "Epoch 112/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4883 - accuracy: 0.8485 - val_loss: 0.4473 - val_accuracy: 0.8648\n",
            "Epoch 113/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4932 - accuracy: 0.8456 - val_loss: 0.4731 - val_accuracy: 0.8544\n",
            "Epoch 114/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4925 - accuracy: 0.8460 - val_loss: 0.4616 - val_accuracy: 0.8610\n",
            "Epoch 115/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4921 - accuracy: 0.8467 - val_loss: 0.4586 - val_accuracy: 0.8621\n",
            "Epoch 116/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4860 - accuracy: 0.8484 - val_loss: 0.4453 - val_accuracy: 0.8661\n",
            "Epoch 117/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4837 - accuracy: 0.8491 - val_loss: 0.4364 - val_accuracy: 0.8666\n",
            "Epoch 118/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4949 - accuracy: 0.8457 - val_loss: 0.4554 - val_accuracy: 0.8603\n",
            "Epoch 119/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4882 - accuracy: 0.8466 - val_loss: 0.4933 - val_accuracy: 0.8473\n",
            "Epoch 120/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4755 - accuracy: 0.8533 - val_loss: 0.4363 - val_accuracy: 0.8662\n",
            "Epoch 121/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4825 - accuracy: 0.8502 - val_loss: 0.4277 - val_accuracy: 0.8704\n",
            "Epoch 122/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4810 - accuracy: 0.8498 - val_loss: 0.4403 - val_accuracy: 0.8663\n",
            "Epoch 123/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4804 - accuracy: 0.8509 - val_loss: 0.4273 - val_accuracy: 0.8707\n",
            "Epoch 124/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4805 - accuracy: 0.8504 - val_loss: 0.4317 - val_accuracy: 0.8705\n",
            "Epoch 125/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4850 - accuracy: 0.8477 - val_loss: 0.4440 - val_accuracy: 0.8620\n",
            "Epoch 126/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4705 - accuracy: 0.8537 - val_loss: 0.4298 - val_accuracy: 0.8716\n",
            "Epoch 127/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4762 - accuracy: 0.8502 - val_loss: 0.4647 - val_accuracy: 0.8592\n",
            "Epoch 128/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4803 - accuracy: 0.8492 - val_loss: 0.5471 - val_accuracy: 0.8361\n",
            "Epoch 129/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4775 - accuracy: 0.8516 - val_loss: 0.4331 - val_accuracy: 0.8683\n",
            "Epoch 130/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4722 - accuracy: 0.8518 - val_loss: 0.5871 - val_accuracy: 0.8247\n",
            "Epoch 131/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4791 - accuracy: 0.8499 - val_loss: 0.4627 - val_accuracy: 0.8614\n",
            "Epoch 132/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4746 - accuracy: 0.8531 - val_loss: 0.5928 - val_accuracy: 0.8219\n",
            "Epoch 133/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4748 - accuracy: 0.8530 - val_loss: 0.4337 - val_accuracy: 0.8704\n",
            "Epoch 134/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4618 - accuracy: 0.8557 - val_loss: 0.4337 - val_accuracy: 0.8701\n",
            "Epoch 135/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4679 - accuracy: 0.8559 - val_loss: 0.4269 - val_accuracy: 0.8717\n",
            "Epoch 136/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4683 - accuracy: 0.8512 - val_loss: 0.4289 - val_accuracy: 0.8708\n",
            "Epoch 137/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4640 - accuracy: 0.8551 - val_loss: 0.4630 - val_accuracy: 0.8598\n",
            "Epoch 138/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4634 - accuracy: 0.8536 - val_loss: 0.4363 - val_accuracy: 0.8691\n",
            "Epoch 139/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4592 - accuracy: 0.8562 - val_loss: 0.4629 - val_accuracy: 0.8612\n",
            "Epoch 140/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4555 - accuracy: 0.8574 - val_loss: 0.4637 - val_accuracy: 0.8614\n",
            "Epoch 141/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4638 - accuracy: 0.8564 - val_loss: 0.4807 - val_accuracy: 0.8547\n",
            "Epoch 142/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4601 - accuracy: 0.8550 - val_loss: 0.4175 - val_accuracy: 0.8757\n",
            "Epoch 143/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4589 - accuracy: 0.8571 - val_loss: 0.4723 - val_accuracy: 0.8583\n",
            "Epoch 144/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4659 - accuracy: 0.8528 - val_loss: 0.4295 - val_accuracy: 0.8721\n",
            "Epoch 145/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4580 - accuracy: 0.8573 - val_loss: 0.4569 - val_accuracy: 0.8611\n",
            "Epoch 146/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4542 - accuracy: 0.8574 - val_loss: 0.4333 - val_accuracy: 0.8704\n",
            "Epoch 147/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4582 - accuracy: 0.8561 - val_loss: 0.4268 - val_accuracy: 0.8727\n",
            "Epoch 148/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4524 - accuracy: 0.8584 - val_loss: 0.4324 - val_accuracy: 0.8711\n",
            "Epoch 149/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4544 - accuracy: 0.8579 - val_loss: 0.4308 - val_accuracy: 0.8706\n",
            "Epoch 150/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4582 - accuracy: 0.8579 - val_loss: 0.4263 - val_accuracy: 0.8728\n",
            "Epoch 151/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4527 - accuracy: 0.8582 - val_loss: 0.4621 - val_accuracy: 0.8583\n",
            "Epoch 152/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4532 - accuracy: 0.8574 - val_loss: 0.4196 - val_accuracy: 0.8756\n",
            "Epoch 153/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4500 - accuracy: 0.8596 - val_loss: 0.4373 - val_accuracy: 0.8677\n",
            "Epoch 154/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4479 - accuracy: 0.8607 - val_loss: 0.5081 - val_accuracy: 0.8476\n",
            "Epoch 155/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4444 - accuracy: 0.8609 - val_loss: 0.4231 - val_accuracy: 0.8736\n",
            "Epoch 156/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4437 - accuracy: 0.8612 - val_loss: 0.4144 - val_accuracy: 0.8769\n",
            "Epoch 157/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4499 - accuracy: 0.8577 - val_loss: 0.4415 - val_accuracy: 0.8703\n",
            "Epoch 158/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4522 - accuracy: 0.8570 - val_loss: 0.4300 - val_accuracy: 0.8723\n",
            "Epoch 159/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4480 - accuracy: 0.8595 - val_loss: 0.4877 - val_accuracy: 0.8500\n",
            "Epoch 160/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4486 - accuracy: 0.8593 - val_loss: 0.4765 - val_accuracy: 0.8562\n",
            "Epoch 161/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4524 - accuracy: 0.8594 - val_loss: 0.4395 - val_accuracy: 0.8682\n",
            "Epoch 162/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4438 - accuracy: 0.8602 - val_loss: 0.4338 - val_accuracy: 0.8718\n",
            "Epoch 163/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4429 - accuracy: 0.8620 - val_loss: 0.4298 - val_accuracy: 0.8722\n",
            "Epoch 164/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4410 - accuracy: 0.8607 - val_loss: 0.6253 - val_accuracy: 0.8149\n",
            "Epoch 165/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4463 - accuracy: 0.8587 - val_loss: 0.4251 - val_accuracy: 0.8718\n",
            "Epoch 166/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4449 - accuracy: 0.8621 - val_loss: 0.4489 - val_accuracy: 0.8651\n",
            "Epoch 167/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4415 - accuracy: 0.8619 - val_loss: 0.4151 - val_accuracy: 0.8756\n",
            "Epoch 168/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4417 - accuracy: 0.8621 - val_loss: 0.4245 - val_accuracy: 0.8728\n",
            "Epoch 169/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4407 - accuracy: 0.8625 - val_loss: 0.5455 - val_accuracy: 0.8366\n",
            "Epoch 170/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4374 - accuracy: 0.8625 - val_loss: 0.4338 - val_accuracy: 0.8714\n",
            "Epoch 171/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4405 - accuracy: 0.8630 - val_loss: 0.5084 - val_accuracy: 0.8457\n",
            "Epoch 172/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4440 - accuracy: 0.8615 - val_loss: 0.4712 - val_accuracy: 0.8592\n",
            "Epoch 173/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4342 - accuracy: 0.8632 - val_loss: 0.4114 - val_accuracy: 0.8786\n",
            "Epoch 174/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4334 - accuracy: 0.8628 - val_loss: 0.4163 - val_accuracy: 0.8749\n",
            "Epoch 175/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4358 - accuracy: 0.8619 - val_loss: 0.4233 - val_accuracy: 0.8714\n",
            "Epoch 176/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4361 - accuracy: 0.8645 - val_loss: 0.4541 - val_accuracy: 0.8642\n",
            "Epoch 177/200\n",
            "657/657 [==============================] - 5s 7ms/step - loss: 0.4298 - accuracy: 0.8647 - val_loss: 0.4324 - val_accuracy: 0.8683\n",
            "Epoch 178/200\n",
            "657/657 [==============================] - 5s 7ms/step - loss: 0.4326 - accuracy: 0.8650 - val_loss: 0.4513 - val_accuracy: 0.8629\n",
            "Epoch 179/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4358 - accuracy: 0.8639 - val_loss: 0.5167 - val_accuracy: 0.8459\n",
            "Epoch 180/200\n",
            "657/657 [==============================] - 5s 7ms/step - loss: 0.4349 - accuracy: 0.8624 - val_loss: 0.5730 - val_accuracy: 0.8329\n",
            "Epoch 181/200\n",
            "657/657 [==============================] - 5s 7ms/step - loss: 0.4295 - accuracy: 0.8653 - val_loss: 0.4601 - val_accuracy: 0.8636\n",
            "Epoch 182/200\n",
            "657/657 [==============================] - 5s 7ms/step - loss: 0.4288 - accuracy: 0.8663 - val_loss: 0.4211 - val_accuracy: 0.8748\n",
            "Epoch 183/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4288 - accuracy: 0.8672 - val_loss: 0.4220 - val_accuracy: 0.8749\n",
            "Epoch 184/200\n",
            "657/657 [==============================] - 8s 12ms/step - loss: 0.4339 - accuracy: 0.8650 - val_loss: 0.4233 - val_accuracy: 0.8739\n",
            "Epoch 185/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4294 - accuracy: 0.8661 - val_loss: 0.4265 - val_accuracy: 0.8724\n",
            "Epoch 186/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4311 - accuracy: 0.8633 - val_loss: 0.4197 - val_accuracy: 0.8737\n",
            "Epoch 187/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4216 - accuracy: 0.8680 - val_loss: 0.4115 - val_accuracy: 0.8765\n",
            "Epoch 188/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4272 - accuracy: 0.8644 - val_loss: 0.4609 - val_accuracy: 0.8596\n",
            "Epoch 189/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4231 - accuracy: 0.8664 - val_loss: 0.4271 - val_accuracy: 0.8713\n",
            "Epoch 190/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4197 - accuracy: 0.8671 - val_loss: 0.4455 - val_accuracy: 0.8664\n",
            "Epoch 191/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4200 - accuracy: 0.8685 - val_loss: 0.4164 - val_accuracy: 0.8747\n",
            "Epoch 192/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4275 - accuracy: 0.8667 - val_loss: 0.4168 - val_accuracy: 0.8749\n",
            "Epoch 193/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4181 - accuracy: 0.8703 - val_loss: 0.4830 - val_accuracy: 0.8546\n",
            "Epoch 194/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4212 - accuracy: 0.8671 - val_loss: 0.4422 - val_accuracy: 0.8667\n",
            "Epoch 195/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4271 - accuracy: 0.8673 - val_loss: 0.4179 - val_accuracy: 0.8763\n",
            "Epoch 196/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4155 - accuracy: 0.8706 - val_loss: 0.4985 - val_accuracy: 0.8524\n",
            "Epoch 197/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4242 - accuracy: 0.8658 - val_loss: 0.4277 - val_accuracy: 0.8718\n",
            "Epoch 198/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4234 - accuracy: 0.8683 - val_loss: 0.5618 - val_accuracy: 0.8337\n",
            "Epoch 199/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4222 - accuracy: 0.8687 - val_loss: 0.4196 - val_accuracy: 0.8733\n",
            "Epoch 200/200\n",
            "657/657 [==============================] - 5s 8ms/step - loss: 0.4162 - accuracy: 0.8696 - val_loss: 0.4126 - val_accuracy: 0.8765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd63d370b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR_2O1hgi_lE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8632dc23-a4a1-424f-8735-1c1b2218ea93"
      },
      "source": [
        "# evaluate model\n",
        "model.evaluate(X_test,y_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
            "563/563 [==============================] - 2s 3ms/step - loss: 0.4126 - accuracy: 0.8765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.41260457038879395, 0.8765000104904175]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL1UboLJ2sUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4fd50c29-d1a5-4eee-a372-3b18f1c71c83"
      },
      "source": [
        "pred = np.round(model.predict(X_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x90z7fZ32ygX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "20b07e73-584e-44d5-c587-2427e768d496"
      },
      "source": [
        "y_class = np.argmax(pred, axis = 1) \n",
        "y_check = np.argmax(y_test, axis = 1) \n",
        "\n",
        "cmatrix = confusion_matrix(y_check, y_class)\n",
        "print(cmatrix)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1730   12    1    3   13    1   17    8    8   21]\n",
            " [ 166 1566    8    8   37    2    2   27    8    4]\n",
            " [ 132   15 1530   22   18   11    1   39   12   23]\n",
            " [ 177   11    9 1365   12   77   10   15   29   14]\n",
            " [ 126   29   11   11 1588    4   10    5    7   21]\n",
            " [ 125    9    3   44    9 1505   40    3   18   12]\n",
            " [ 156    8    7    8   19   47 1537    9   35    6]\n",
            " [ 109   39   16   12    9    5    5 1600    6    7]\n",
            " [ 195   15    9   19    8   10   51    4 1471   30]\n",
            " [ 184   18   10   16   16   22    5   10   20 1503]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkfF__Hr29lN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "42b37387-a2c9-4ed9-cfa4-cb8d03f079f1"
      },
      "source": [
        "print(metrics.classification_report(y_test, pred))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.89      0.91      1814\n",
            "           1       0.91      0.86      0.88      1828\n",
            "           2       0.95      0.85      0.90      1803\n",
            "           3       0.91      0.79      0.85      1719\n",
            "           4       0.92      0.88      0.90      1812\n",
            "           5       0.89      0.85      0.87      1768\n",
            "           6       0.92      0.84      0.88      1832\n",
            "           7       0.93      0.88      0.91      1808\n",
            "           8       0.91      0.81      0.86      1812\n",
            "           9       0.92      0.83      0.87      1804\n",
            "\n",
            "   micro avg       0.92      0.85      0.88     18000\n",
            "   macro avg       0.92      0.85      0.88     18000\n",
            "weighted avg       0.92      0.85      0.88     18000\n",
            " samples avg       0.85      0.85      0.85     18000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNiHTRna391m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}