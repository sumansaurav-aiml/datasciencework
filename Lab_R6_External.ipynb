{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_ExternalLab_AIML-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YYk8NG3yOIT9"
      },
      "source": [
        "### A MNIST-like fashion product database\n",
        "\n",
        "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tFO6PuxzOIT_"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efNjNImfOIUC",
        "outputId": "10b6add0-8e33-4e62-b734-e5c07bbfbda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cb336b966eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'set_random_seed'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9C4aAIGOIUH",
        "outputId": "784945cc-a19c-4de7-8737-ab3a4b9ed531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcoZBStrOIUQ"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XA1WsFSeOIUS",
        "outputId": "72e08f75-a974-4101-b93c-c65a013e8e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import keras"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnbx7TyQOIUY",
        "outputId": "1b61bc3d-7da1-4930-cc4d-a36c472c6f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UbiHj5YPOIUc",
        "outputId": "1596992f-397c-412b-ae8c-d9ca09e976b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(testY[0:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZbZR75bmQ3M",
        "colab_type": "code",
        "outputId": "3f294cdf-757b-403c-d8de-fa812aafa8be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "source": [
        "print(testX[0:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lDAYzkwyOIUj"
      },
      "source": [
        "### Convert both training and testing labels into one-hot vectors.\n",
        "\n",
        "**Hint:** check **tf.keras.utils.to_categorical()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vBlfYlANOIUk",
        "colab": {}
      },
      "source": [
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RHV3b9mzOIUq",
        "outputId": "6c6c08db-3ecd-4888-a35a-1f1ba2ac628f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "print(trainY.shape)\n",
        "print('First 5 examples now are: ', trainY[0:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n",
            "First 5 examples now are:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwhQ8e7VOIUw"
      },
      "source": [
        "### Visualize the data\n",
        "\n",
        "Plot first 10 images in the triaining set and their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvDML2OoOIUx",
        "outputId": "d2f16e10-abaf-4b8f-bfe1-705fdd707fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "w=10\n",
        "h=10\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 10\n",
        "rows = 1\n",
        "for i in range(1, columns*rows +1):\n",
        "    img = testX[i]\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    print(testY[i])\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4TbJGeSOIU4"
      },
      "source": [
        "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ac06XZZTOIU6",
        "colab": {}
      },
      "source": [
        "# Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "# Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3hQpLv3aOIU_"
      },
      "source": [
        "### Execute the model using model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O59C_-IgOIVB",
        "outputId": "0576ef86-b841-4514-aa94-7424ac8c2562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=50,batch_size = trainX.shape[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 187.1154 - accuracy: 0.1647 - val_loss: 6227.0454 - val_accuracy: 0.1273\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 6244.2217 - accuracy: 0.1265 - val_loss: 12485.8340 - val_accuracy: 0.3503\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 12435.3516 - accuracy: 0.3548 - val_loss: 15950.3389 - val_accuracy: 0.1873\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 15859.9023 - accuracy: 0.1910 - val_loss: 18875.6797 - val_accuracy: 0.2223\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 18776.9766 - accuracy: 0.2216 - val_loss: 14434.6045 - val_accuracy: 0.2837\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 14353.6289 - accuracy: 0.2864 - val_loss: 14019.6699 - val_accuracy: 0.3115\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 13949.3789 - accuracy: 0.3116 - val_loss: 7908.7090 - val_accuracy: 0.2878\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 7936.9438 - accuracy: 0.2884 - val_loss: 6527.4746 - val_accuracy: 0.3669\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 6491.2280 - accuracy: 0.3647 - val_loss: 8271.3438 - val_accuracy: 0.3246\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 8198.5947 - accuracy: 0.3251 - val_loss: 5730.5889 - val_accuracy: 0.5439\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 5683.8052 - accuracy: 0.5451 - val_loss: 5604.5850 - val_accuracy: 0.5501\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 5553.9966 - accuracy: 0.5584 - val_loss: 4284.4170 - val_accuracy: 0.6093\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 4233.8174 - accuracy: 0.6161 - val_loss: 2059.0266 - val_accuracy: 0.5093\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2032.5278 - accuracy: 0.5127 - val_loss: 2819.2925 - val_accuracy: 0.5723\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2765.3562 - accuracy: 0.5840 - val_loss: 1874.8503 - val_accuracy: 0.5736\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1818.3749 - accuracy: 0.5832 - val_loss: 3649.2537 - val_accuracy: 0.4732\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 3553.0967 - accuracy: 0.4788 - val_loss: 4895.5664 - val_accuracy: 0.5874\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 4853.8003 - accuracy: 0.5954 - val_loss: 5137.9766 - val_accuracy: 0.5880\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 5111.9067 - accuracy: 0.5974 - val_loss: 3146.0115 - val_accuracy: 0.5174\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 3088.0784 - accuracy: 0.5242 - val_loss: 3048.6875 - val_accuracy: 0.5030\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2971.0459 - accuracy: 0.5173 - val_loss: 3803.4563 - val_accuracy: 0.5183\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 3731.7834 - accuracy: 0.5301 - val_loss: 2275.7251 - val_accuracy: 0.5895\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 2224.3489 - accuracy: 0.5956 - val_loss: 2189.8813 - val_accuracy: 0.5930\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2145.6963 - accuracy: 0.6009 - val_loss: 4395.3706 - val_accuracy: 0.5310\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 4271.3262 - accuracy: 0.5400 - val_loss: 4856.1226 - val_accuracy: 0.6383\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 4788.0688 - accuracy: 0.6430 - val_loss: 2767.6123 - val_accuracy: 0.6569\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2737.4597 - accuracy: 0.6572 - val_loss: 2260.9475 - val_accuracy: 0.6389\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2209.7595 - accuracy: 0.6400 - val_loss: 2705.2517 - val_accuracy: 0.5685\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2636.3611 - accuracy: 0.5812 - val_loss: 4397.8335 - val_accuracy: 0.5734\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 4327.3022 - accuracy: 0.5829 - val_loss: 4510.2246 - val_accuracy: 0.6581\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 4431.5776 - accuracy: 0.6614 - val_loss: 2036.6542 - val_accuracy: 0.6462\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1984.4896 - accuracy: 0.6517 - val_loss: 1971.9872 - val_accuracy: 0.7172\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1939.5754 - accuracy: 0.7243 - val_loss: 1615.5571 - val_accuracy: 0.6794\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1567.0492 - accuracy: 0.6819 - val_loss: 2453.2688 - val_accuracy: 0.6274\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2389.0198 - accuracy: 0.6330 - val_loss: 2199.6396 - val_accuracy: 0.6743\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2152.8606 - accuracy: 0.6817 - val_loss: 1918.8011 - val_accuracy: 0.6140\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1850.6057 - accuracy: 0.6163 - val_loss: 2910.1160 - val_accuracy: 0.6222\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 2846.9028 - accuracy: 0.6275 - val_loss: 1229.6145 - val_accuracy: 0.6733\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1178.0682 - accuracy: 0.6837 - val_loss: 1598.0673 - val_accuracy: 0.6916\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1543.2512 - accuracy: 0.7032 - val_loss: 1914.2780 - val_accuracy: 0.6914\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1832.3370 - accuracy: 0.7019 - val_loss: 1061.8621 - val_accuracy: 0.7070\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1019.7924 - accuracy: 0.7150 - val_loss: 2358.8398 - val_accuracy: 0.6172\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2248.5317 - accuracy: 0.6287 - val_loss: 2982.8350 - val_accuracy: 0.6820\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2929.8718 - accuracy: 0.6908 - val_loss: 3269.1348 - val_accuracy: 0.6455\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 3216.6516 - accuracy: 0.6534 - val_loss: 2198.0452 - val_accuracy: 0.6991\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 2134.3347 - accuracy: 0.7039 - val_loss: 2576.8330 - val_accuracy: 0.6182\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2484.7886 - accuracy: 0.6243 - val_loss: 3344.3120 - val_accuracy: 0.6839\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 3283.9402 - accuracy: 0.6936 - val_loss: 3683.7329 - val_accuracy: 0.6730\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 3605.3647 - accuracy: 0.6784 - val_loss: 1655.9750 - val_accuracy: 0.6994\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1611.8118 - accuracy: 0.7046 - val_loss: 2284.2224 - val_accuracy: 0.6342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1fef9b2358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JdzDtGwDOIVF"
      },
      "source": [
        "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kndfpdidOIVI",
        "colab": {}
      },
      "source": [
        "# Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mwk3T5LJOIVN"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JNLR8tcBOIVP",
        "outputId": "2db92c8e-212d-4372-aa7b-72e034a97d88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=50,batch_size = trainX.shape[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 3.1157 - accuracy: 0.0990 - val_loss: 28.8096 - val_accuracy: 0.1528\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 2.7880 - accuracy: 0.1361 - val_loss: 17.4057 - val_accuracy: 0.1776\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.5205 - accuracy: 0.1894 - val_loss: 12.2768 - val_accuracy: 0.2046\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.3020 - accuracy: 0.2452 - val_loss: 9.3644 - val_accuracy: 0.2331\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.1227 - accuracy: 0.2982 - val_loss: 7.5293 - val_accuracy: 0.2613\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.9741 - accuracy: 0.3444 - val_loss: 6.2810 - val_accuracy: 0.2837\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.8497 - accuracy: 0.3843 - val_loss: 5.3809 - val_accuracy: 0.3022\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.7444 - accuracy: 0.4173 - val_loss: 4.7053 - val_accuracy: 0.3209\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6546 - accuracy: 0.4458 - val_loss: 4.1837 - val_accuracy: 0.3427\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.5774 - accuracy: 0.4692 - val_loss: 3.7722 - val_accuracy: 0.3623\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.5106 - accuracy: 0.4902 - val_loss: 3.4418 - val_accuracy: 0.3835\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.4524 - accuracy: 0.5098 - val_loss: 3.1727 - val_accuracy: 0.4009\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.4014 - accuracy: 0.5280 - val_loss: 2.9506 - val_accuracy: 0.4196\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3565 - accuracy: 0.5434 - val_loss: 2.7649 - val_accuracy: 0.4366\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.3167 - accuracy: 0.5570 - val_loss: 2.6076 - val_accuracy: 0.4533\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.2813 - accuracy: 0.5698 - val_loss: 2.4726 - val_accuracy: 0.4690\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.2496 - accuracy: 0.5809 - val_loss: 2.3555 - val_accuracy: 0.4855\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.2211 - accuracy: 0.5911 - val_loss: 2.2529 - val_accuracy: 0.5002\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.1952 - accuracy: 0.5997 - val_loss: 2.1620 - val_accuracy: 0.5110\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.1717 - accuracy: 0.6079 - val_loss: 2.0809 - val_accuracy: 0.5236\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.1502 - accuracy: 0.6155 - val_loss: 2.0078 - val_accuracy: 0.5329\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.1305 - accuracy: 0.6224 - val_loss: 1.9415 - val_accuracy: 0.5423\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.1123 - accuracy: 0.6289 - val_loss: 1.8811 - val_accuracy: 0.5500\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.0954 - accuracy: 0.6341 - val_loss: 1.8257 - val_accuracy: 0.5588\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.0798 - accuracy: 0.6395 - val_loss: 1.7746 - val_accuracy: 0.5649\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.0652 - accuracy: 0.6447 - val_loss: 1.7274 - val_accuracy: 0.5717\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.0516 - accuracy: 0.6494 - val_loss: 1.6835 - val_accuracy: 0.5778\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.0388 - accuracy: 0.6538 - val_loss: 1.6427 - val_accuracy: 0.5826\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.0268 - accuracy: 0.6574 - val_loss: 1.6045 - val_accuracy: 0.5878\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.0155 - accuracy: 0.6609 - val_loss: 1.5687 - val_accuracy: 0.5927\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.0048 - accuracy: 0.6646 - val_loss: 1.5351 - val_accuracy: 0.5964\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.9947 - accuracy: 0.6672 - val_loss: 1.5035 - val_accuracy: 0.6000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.9851 - accuracy: 0.6702 - val_loss: 1.4737 - val_accuracy: 0.6034\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9760 - accuracy: 0.6732 - val_loss: 1.4456 - val_accuracy: 0.6083\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.9673 - accuracy: 0.6758 - val_loss: 1.4190 - val_accuracy: 0.6125\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.9591 - accuracy: 0.6781 - val_loss: 1.3937 - val_accuracy: 0.6164\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.9512 - accuracy: 0.6806 - val_loss: 1.3697 - val_accuracy: 0.6197\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.9436 - accuracy: 0.6830 - val_loss: 1.3470 - val_accuracy: 0.6229\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9364 - accuracy: 0.6850 - val_loss: 1.3253 - val_accuracy: 0.6249\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.9295 - accuracy: 0.6870 - val_loss: 1.3046 - val_accuracy: 0.6282\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.9228 - accuracy: 0.6888 - val_loss: 1.2849 - val_accuracy: 0.6320\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.9165 - accuracy: 0.6907 - val_loss: 1.2661 - val_accuracy: 0.6345\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.9103 - accuracy: 0.6924 - val_loss: 1.2481 - val_accuracy: 0.6377\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.9044 - accuracy: 0.6941 - val_loss: 1.2309 - val_accuracy: 0.6395\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.8987 - accuracy: 0.6957 - val_loss: 1.2144 - val_accuracy: 0.6419\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8932 - accuracy: 0.6974 - val_loss: 1.1986 - val_accuracy: 0.6440\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8879 - accuracy: 0.6989 - val_loss: 1.1834 - val_accuracy: 0.6465\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8828 - accuracy: 0.7003 - val_loss: 1.1688 - val_accuracy: 0.6485\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8778 - accuracy: 0.7018 - val_loss: 1.1548 - val_accuracy: 0.6516\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.8730 - accuracy: 0.7036 - val_loss: 1.1413 - val_accuracy: 0.6534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1feb30edd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Py-KwkmjOIVU"
      },
      "source": [
        "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLXUE9jWOIVV",
        "colab": {}
      },
      "source": [
        "# Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pJUqA5T4OIVc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d50cc62d-25cd-4977-b656-d5b37612ea37"
      },
      "source": [
        "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=200,batch_size = trainX.shape[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 2.6791 - accuracy: 0.1479 - val_loss: 22.3805 - val_accuracy: 0.1604\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.6553 - accuracy: 0.1520 - val_loss: 15.7554 - val_accuracy: 0.1632\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 2.6320 - accuracy: 0.1564 - val_loss: 12.7538 - val_accuracy: 0.1645\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.6091 - accuracy: 0.1606 - val_loss: 10.9400 - val_accuracy: 0.1662\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.5867 - accuracy: 0.1652 - val_loss: 9.6897 - val_accuracy: 0.1687\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.5647 - accuracy: 0.1699 - val_loss: 8.7594 - val_accuracy: 0.1711\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 2.5431 - accuracy: 0.1748 - val_loss: 8.0318 - val_accuracy: 0.1730\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 2.5220 - accuracy: 0.1796 - val_loss: 7.4423 - val_accuracy: 0.1745\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 2.5012 - accuracy: 0.1844 - val_loss: 6.9521 - val_accuracy: 0.1765\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.4809 - accuracy: 0.1890 - val_loss: 6.5363 - val_accuracy: 0.1788\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.4610 - accuracy: 0.1937 - val_loss: 6.1777 - val_accuracy: 0.1811\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.4414 - accuracy: 0.1979 - val_loss: 5.8645 - val_accuracy: 0.1827\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.4222 - accuracy: 0.2025 - val_loss: 5.5879 - val_accuracy: 0.1852\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.4033 - accuracy: 0.2071 - val_loss: 5.3415 - val_accuracy: 0.1892\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.3849 - accuracy: 0.2116 - val_loss: 5.1202 - val_accuracy: 0.1919\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 2.3667 - accuracy: 0.2159 - val_loss: 4.9202 - val_accuracy: 0.1942\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.3489 - accuracy: 0.2199 - val_loss: 4.7382 - val_accuracy: 0.1962\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.3314 - accuracy: 0.2244 - val_loss: 4.5720 - val_accuracy: 0.1978\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.3142 - accuracy: 0.2286 - val_loss: 4.4193 - val_accuracy: 0.2001\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.2973 - accuracy: 0.2329 - val_loss: 4.2785 - val_accuracy: 0.2032\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 2.2808 - accuracy: 0.2370 - val_loss: 4.1482 - val_accuracy: 0.2054\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.2645 - accuracy: 0.2414 - val_loss: 4.0272 - val_accuracy: 0.2078\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 2.2485 - accuracy: 0.2459 - val_loss: 3.9146 - val_accuracy: 0.2099\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 2.2328 - accuracy: 0.2497 - val_loss: 3.8094 - val_accuracy: 0.2134\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 2.2173 - accuracy: 0.2531 - val_loss: 3.7108 - val_accuracy: 0.2175\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.2022 - accuracy: 0.2573 - val_loss: 3.6184 - val_accuracy: 0.2193\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.1872 - accuracy: 0.2612 - val_loss: 3.5314 - val_accuracy: 0.2223\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.1726 - accuracy: 0.2650 - val_loss: 3.4495 - val_accuracy: 0.2248\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.1582 - accuracy: 0.2686 - val_loss: 3.3721 - val_accuracy: 0.2274\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 2.1440 - accuracy: 0.2722 - val_loss: 3.2989 - val_accuracy: 0.2304\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.1300 - accuracy: 0.2755 - val_loss: 3.2295 - val_accuracy: 0.2328\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 2.1163 - accuracy: 0.2790 - val_loss: 3.1637 - val_accuracy: 0.2353\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 2.1028 - accuracy: 0.2822 - val_loss: 3.1012 - val_accuracy: 0.2379\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.0896 - accuracy: 0.2858 - val_loss: 3.0417 - val_accuracy: 0.2417\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 2.0765 - accuracy: 0.2887 - val_loss: 2.9850 - val_accuracy: 0.2450\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.0637 - accuracy: 0.2918 - val_loss: 2.9309 - val_accuracy: 0.2491\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.0510 - accuracy: 0.2949 - val_loss: 2.8793 - val_accuracy: 0.2520\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.0386 - accuracy: 0.2978 - val_loss: 2.8299 - val_accuracy: 0.2551\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.0263 - accuracy: 0.3008 - val_loss: 2.7827 - val_accuracy: 0.2574\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 2.0143 - accuracy: 0.3038 - val_loss: 2.7375 - val_accuracy: 0.2603\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 2.0024 - accuracy: 0.3072 - val_loss: 2.6941 - val_accuracy: 0.2629\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.9907 - accuracy: 0.3106 - val_loss: 2.6524 - val_accuracy: 0.2655\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.9792 - accuracy: 0.3132 - val_loss: 2.6125 - val_accuracy: 0.2682\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.9679 - accuracy: 0.3163 - val_loss: 2.5740 - val_accuracy: 0.2707\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.9568 - accuracy: 0.3196 - val_loss: 2.5371 - val_accuracy: 0.2744\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.9458 - accuracy: 0.3223 - val_loss: 2.5015 - val_accuracy: 0.2781\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.9350 - accuracy: 0.3250 - val_loss: 2.4672 - val_accuracy: 0.2810\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.9243 - accuracy: 0.3276 - val_loss: 2.4342 - val_accuracy: 0.2848\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.9138 - accuracy: 0.3298 - val_loss: 2.4024 - val_accuracy: 0.2887\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.9034 - accuracy: 0.3325 - val_loss: 2.3716 - val_accuracy: 0.2905\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.8932 - accuracy: 0.3354 - val_loss: 2.3419 - val_accuracy: 0.2930\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 1.8832 - accuracy: 0.3384 - val_loss: 2.3133 - val_accuracy: 0.2966\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.8733 - accuracy: 0.3411 - val_loss: 2.2855 - val_accuracy: 0.2988\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.8635 - accuracy: 0.3440 - val_loss: 2.2587 - val_accuracy: 0.3034\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.8539 - accuracy: 0.3467 - val_loss: 2.2328 - val_accuracy: 0.3076\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.8444 - accuracy: 0.3494 - val_loss: 2.2077 - val_accuracy: 0.3114\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.8351 - accuracy: 0.3521 - val_loss: 2.1834 - val_accuracy: 0.3156\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.8259 - accuracy: 0.3546 - val_loss: 2.1598 - val_accuracy: 0.3199\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.8168 - accuracy: 0.3572 - val_loss: 2.1370 - val_accuracy: 0.3227\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.8078 - accuracy: 0.3597 - val_loss: 2.1148 - val_accuracy: 0.3266\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.7990 - accuracy: 0.3620 - val_loss: 2.0933 - val_accuracy: 0.3300\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.7903 - accuracy: 0.3647 - val_loss: 2.0725 - val_accuracy: 0.3339\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.7817 - accuracy: 0.3668 - val_loss: 2.0522 - val_accuracy: 0.3360\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.7732 - accuracy: 0.3691 - val_loss: 2.0326 - val_accuracy: 0.3388\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.7648 - accuracy: 0.3718 - val_loss: 2.0135 - val_accuracy: 0.3427\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.7566 - accuracy: 0.3744 - val_loss: 1.9949 - val_accuracy: 0.3462\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.7484 - accuracy: 0.3770 - val_loss: 1.9769 - val_accuracy: 0.3499\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.7404 - accuracy: 0.3795 - val_loss: 1.9594 - val_accuracy: 0.3533\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.7325 - accuracy: 0.3817 - val_loss: 1.9423 - val_accuracy: 0.3567\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 1.7247 - accuracy: 0.3841 - val_loss: 1.9257 - val_accuracy: 0.3614\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.7170 - accuracy: 0.3858 - val_loss: 1.9095 - val_accuracy: 0.3649\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.7093 - accuracy: 0.3880 - val_loss: 1.8938 - val_accuracy: 0.3674\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.7018 - accuracy: 0.3903 - val_loss: 1.8785 - val_accuracy: 0.3708\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.6944 - accuracy: 0.3923 - val_loss: 1.8636 - val_accuracy: 0.3738\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.6871 - accuracy: 0.3946 - val_loss: 1.8490 - val_accuracy: 0.3769\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.6799 - accuracy: 0.3970 - val_loss: 1.8349 - val_accuracy: 0.3802\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6727 - accuracy: 0.3991 - val_loss: 1.8210 - val_accuracy: 0.3835\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.6657 - accuracy: 0.4013 - val_loss: 1.8076 - val_accuracy: 0.3867\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.6588 - accuracy: 0.4033 - val_loss: 1.7944 - val_accuracy: 0.3906\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 1.6519 - accuracy: 0.4056 - val_loss: 1.7816 - val_accuracy: 0.3929\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.6451 - accuracy: 0.4079 - val_loss: 1.7691 - val_accuracy: 0.3955\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6384 - accuracy: 0.4099 - val_loss: 1.7569 - val_accuracy: 0.3988\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.6318 - accuracy: 0.4119 - val_loss: 1.7450 - val_accuracy: 0.4021\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6253 - accuracy: 0.4139 - val_loss: 1.7334 - val_accuracy: 0.4057\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6189 - accuracy: 0.4163 - val_loss: 1.7221 - val_accuracy: 0.4086\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6125 - accuracy: 0.4184 - val_loss: 1.7110 - val_accuracy: 0.4108\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.6062 - accuracy: 0.4206 - val_loss: 1.7001 - val_accuracy: 0.4139\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.6000 - accuracy: 0.4228 - val_loss: 1.6896 - val_accuracy: 0.4183\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5939 - accuracy: 0.4250 - val_loss: 1.6792 - val_accuracy: 0.4207\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 1.5878 - accuracy: 0.4271 - val_loss: 1.6691 - val_accuracy: 0.4243\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.5818 - accuracy: 0.4294 - val_loss: 1.6592 - val_accuracy: 0.4275\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.5759 - accuracy: 0.4315 - val_loss: 1.6496 - val_accuracy: 0.4302\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5701 - accuracy: 0.4334 - val_loss: 1.6401 - val_accuracy: 0.4327\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5643 - accuracy: 0.4356 - val_loss: 1.6309 - val_accuracy: 0.4347\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5586 - accuracy: 0.4377 - val_loss: 1.6218 - val_accuracy: 0.4378\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5530 - accuracy: 0.4397 - val_loss: 1.6130 - val_accuracy: 0.4408\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5474 - accuracy: 0.4417 - val_loss: 1.6043 - val_accuracy: 0.4434\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.5419 - accuracy: 0.4438 - val_loss: 1.5958 - val_accuracy: 0.4459\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5365 - accuracy: 0.4455 - val_loss: 1.5875 - val_accuracy: 0.4478\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.5311 - accuracy: 0.4477 - val_loss: 1.5794 - val_accuracy: 0.4504\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.5258 - accuracy: 0.4497 - val_loss: 1.5714 - val_accuracy: 0.4525\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.5205 - accuracy: 0.4516 - val_loss: 1.5636 - val_accuracy: 0.4546\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5154 - accuracy: 0.4534 - val_loss: 1.5560 - val_accuracy: 0.4563\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5102 - accuracy: 0.4551 - val_loss: 1.5485 - val_accuracy: 0.4582\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.5052 - accuracy: 0.4567 - val_loss: 1.5412 - val_accuracy: 0.4602\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.5001 - accuracy: 0.4587 - val_loss: 1.5340 - val_accuracy: 0.4629\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.4952 - accuracy: 0.4604 - val_loss: 1.5269 - val_accuracy: 0.4643\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4903 - accuracy: 0.4619 - val_loss: 1.5200 - val_accuracy: 0.4657\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 1.4854 - accuracy: 0.4640 - val_loss: 1.5132 - val_accuracy: 0.4682\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4806 - accuracy: 0.4654 - val_loss: 1.5066 - val_accuracy: 0.4705\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4759 - accuracy: 0.4673 - val_loss: 1.5000 - val_accuracy: 0.4733\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.4712 - accuracy: 0.4686 - val_loss: 1.4936 - val_accuracy: 0.4752\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.4666 - accuracy: 0.4705 - val_loss: 1.4874 - val_accuracy: 0.4768\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.4620 - accuracy: 0.4722 - val_loss: 1.4812 - val_accuracy: 0.4792\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.4575 - accuracy: 0.4739 - val_loss: 1.4751 - val_accuracy: 0.4799\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.4530 - accuracy: 0.4758 - val_loss: 1.4692 - val_accuracy: 0.4823\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4486 - accuracy: 0.4774 - val_loss: 1.4634 - val_accuracy: 0.4835\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4442 - accuracy: 0.4791 - val_loss: 1.4576 - val_accuracy: 0.4843\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.4398 - accuracy: 0.4807 - val_loss: 1.4520 - val_accuracy: 0.4857\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4356 - accuracy: 0.4824 - val_loss: 1.4465 - val_accuracy: 0.4871\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.4313 - accuracy: 0.4841 - val_loss: 1.4411 - val_accuracy: 0.4885\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.4271 - accuracy: 0.4856 - val_loss: 1.4357 - val_accuracy: 0.4907\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.4230 - accuracy: 0.4872 - val_loss: 1.4305 - val_accuracy: 0.4928\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.4188 - accuracy: 0.4884 - val_loss: 1.4254 - val_accuracy: 0.4939\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 1.4148 - accuracy: 0.4902 - val_loss: 1.4203 - val_accuracy: 0.4959\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.4108 - accuracy: 0.4921 - val_loss: 1.4153 - val_accuracy: 0.4978\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.4068 - accuracy: 0.4940 - val_loss: 1.4104 - val_accuracy: 0.4992\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.4028 - accuracy: 0.4956 - val_loss: 1.4056 - val_accuracy: 0.5002\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.3989 - accuracy: 0.4968 - val_loss: 1.4009 - val_accuracy: 0.5026\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.3951 - accuracy: 0.4983 - val_loss: 1.3962 - val_accuracy: 0.5043\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 1.3912 - accuracy: 0.5000 - val_loss: 1.3917 - val_accuracy: 0.5054\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.3875 - accuracy: 0.5018 - val_loss: 1.3872 - val_accuracy: 0.5073\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.3837 - accuracy: 0.5033 - val_loss: 1.3827 - val_accuracy: 0.5084\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.3800 - accuracy: 0.5050 - val_loss: 1.3784 - val_accuracy: 0.5099\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.3763 - accuracy: 0.5064 - val_loss: 1.3741 - val_accuracy: 0.5115\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.3727 - accuracy: 0.5080 - val_loss: 1.3699 - val_accuracy: 0.5130\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.3691 - accuracy: 0.5092 - val_loss: 1.3657 - val_accuracy: 0.5140\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.3656 - accuracy: 0.5105 - val_loss: 1.3616 - val_accuracy: 0.5149\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.3620 - accuracy: 0.5119 - val_loss: 1.3576 - val_accuracy: 0.5164\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.3586 - accuracy: 0.5132 - val_loss: 1.3536 - val_accuracy: 0.5174\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3551 - accuracy: 0.5144 - val_loss: 1.3497 - val_accuracy: 0.5195\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.3517 - accuracy: 0.5159 - val_loss: 1.3459 - val_accuracy: 0.5208\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3483 - accuracy: 0.5172 - val_loss: 1.3421 - val_accuracy: 0.5218\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.3449 - accuracy: 0.5188 - val_loss: 1.3383 - val_accuracy: 0.5232\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.3416 - accuracy: 0.5202 - val_loss: 1.3347 - val_accuracy: 0.5245\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 1.3383 - accuracy: 0.5215 - val_loss: 1.3310 - val_accuracy: 0.5254\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 1.3351 - accuracy: 0.5227 - val_loss: 1.3274 - val_accuracy: 0.5262\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.3318 - accuracy: 0.5239 - val_loss: 1.3239 - val_accuracy: 0.5278\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.3287 - accuracy: 0.5249 - val_loss: 1.3204 - val_accuracy: 0.5293\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.3255 - accuracy: 0.5260 - val_loss: 1.3170 - val_accuracy: 0.5303\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.3223 - accuracy: 0.5271 - val_loss: 1.3136 - val_accuracy: 0.5315\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.3192 - accuracy: 0.5283 - val_loss: 1.3103 - val_accuracy: 0.5331\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3162 - accuracy: 0.5293 - val_loss: 1.3070 - val_accuracy: 0.5346\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.3131 - accuracy: 0.5306 - val_loss: 1.3038 - val_accuracy: 0.5357\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.3101 - accuracy: 0.5317 - val_loss: 1.3006 - val_accuracy: 0.5368\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3071 - accuracy: 0.5328 - val_loss: 1.2974 - val_accuracy: 0.5385\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.3041 - accuracy: 0.5341 - val_loss: 1.2943 - val_accuracy: 0.5400\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.3012 - accuracy: 0.5352 - val_loss: 1.2912 - val_accuracy: 0.5410\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.2983 - accuracy: 0.5363 - val_loss: 1.2882 - val_accuracy: 0.5419\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.2954 - accuracy: 0.5374 - val_loss: 1.2852 - val_accuracy: 0.5432\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 1.2925 - accuracy: 0.5388 - val_loss: 1.2822 - val_accuracy: 0.5441\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.2897 - accuracy: 0.5402 - val_loss: 1.2793 - val_accuracy: 0.5454\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.2869 - accuracy: 0.5412 - val_loss: 1.2764 - val_accuracy: 0.5463\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.2841 - accuracy: 0.5422 - val_loss: 1.2736 - val_accuracy: 0.5471\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.2814 - accuracy: 0.5430 - val_loss: 1.2708 - val_accuracy: 0.5482\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.2786 - accuracy: 0.5443 - val_loss: 1.2680 - val_accuracy: 0.5494\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.2759 - accuracy: 0.5454 - val_loss: 1.2652 - val_accuracy: 0.5511\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.2732 - accuracy: 0.5465 - val_loss: 1.2625 - val_accuracy: 0.5520\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.2706 - accuracy: 0.5476 - val_loss: 1.2599 - val_accuracy: 0.5529\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.2679 - accuracy: 0.5487 - val_loss: 1.2572 - val_accuracy: 0.5540\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.2653 - accuracy: 0.5500 - val_loss: 1.2546 - val_accuracy: 0.5553\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.2627 - accuracy: 0.5510 - val_loss: 1.2520 - val_accuracy: 0.5563\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.2601 - accuracy: 0.5519 - val_loss: 1.2495 - val_accuracy: 0.5570\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.2576 - accuracy: 0.5530 - val_loss: 1.2469 - val_accuracy: 0.5581\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.2551 - accuracy: 0.5541 - val_loss: 1.2444 - val_accuracy: 0.5592\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.2525 - accuracy: 0.5548 - val_loss: 1.2420 - val_accuracy: 0.5602\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.2501 - accuracy: 0.5557 - val_loss: 1.2395 - val_accuracy: 0.5614\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.2476 - accuracy: 0.5565 - val_loss: 1.2371 - val_accuracy: 0.5622\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.2451 - accuracy: 0.5576 - val_loss: 1.2347 - val_accuracy: 0.5631\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.2427 - accuracy: 0.5586 - val_loss: 1.2324 - val_accuracy: 0.5647\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.2403 - accuracy: 0.5597 - val_loss: 1.2300 - val_accuracy: 0.5655\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.2379 - accuracy: 0.5604 - val_loss: 1.2277 - val_accuracy: 0.5661\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.2356 - accuracy: 0.5612 - val_loss: 1.2254 - val_accuracy: 0.5670\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.2332 - accuracy: 0.5620 - val_loss: 1.2232 - val_accuracy: 0.5679\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.2309 - accuracy: 0.5628 - val_loss: 1.2209 - val_accuracy: 0.5683\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.2286 - accuracy: 0.5636 - val_loss: 1.2187 - val_accuracy: 0.5692\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.2263 - accuracy: 0.5645 - val_loss: 1.2165 - val_accuracy: 0.5707\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.2240 - accuracy: 0.5655 - val_loss: 1.2144 - val_accuracy: 0.5719\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.2218 - accuracy: 0.5665 - val_loss: 1.2122 - val_accuracy: 0.5725\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.2195 - accuracy: 0.5674 - val_loss: 1.2101 - val_accuracy: 0.5735\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.2173 - accuracy: 0.5682 - val_loss: 1.2080 - val_accuracy: 0.5738\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.2151 - accuracy: 0.5691 - val_loss: 1.2059 - val_accuracy: 0.5749\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.2129 - accuracy: 0.5701 - val_loss: 1.2038 - val_accuracy: 0.5752\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.2107 - accuracy: 0.5709 - val_loss: 1.2018 - val_accuracy: 0.5758\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.2086 - accuracy: 0.5717 - val_loss: 1.1998 - val_accuracy: 0.5767\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.2065 - accuracy: 0.5726 - val_loss: 1.1978 - val_accuracy: 0.5774\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.2043 - accuracy: 0.5733 - val_loss: 1.1958 - val_accuracy: 0.5789\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.2022 - accuracy: 0.5741 - val_loss: 1.1938 - val_accuracy: 0.5800\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.2001 - accuracy: 0.5750 - val_loss: 1.1919 - val_accuracy: 0.5805\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.1981 - accuracy: 0.5760 - val_loss: 1.1899 - val_accuracy: 0.5814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1fec246ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j9CSqKvpOIVk"
      },
      "source": [
        "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGAad54JOIVm",
        "colab": {}
      },
      "source": [
        "# Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "\n",
        "# Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X-O-fFxnOIVt",
        "colab": {}
      },
      "source": [
        "sgd = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BiP7IL52OIVw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nr2YsZV0OIV0"
      },
      "source": [
        "## Review model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h4ojW6-oOIV2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "d1426387-a3ce-4b74-e382-e491da134739"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_5 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 93,546\n",
            "Trainable params: 91,578\n",
            "Non-trainable params: 1,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gfFGmbZLOIV5"
      },
      "source": [
        "### Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIkbMEN5OIV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1030734-0faa-449a-9709-c96c18ef6d78"
      },
      "source": [
        "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=200,batch_size = trainX.shape[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 3.1429 - accuracy: 0.0792 - val_loss: 2.4536 - val_accuracy: 0.1000\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 2.0978 - accuracy: 0.2867 - val_loss: 2.4227 - val_accuracy: 0.1000\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.6508 - accuracy: 0.4344 - val_loss: 2.4019 - val_accuracy: 0.1001\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 1.4113 - accuracy: 0.5266 - val_loss: 2.3870 - val_accuracy: 0.1002\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 1.2669 - accuracy: 0.5814 - val_loss: 2.3746 - val_accuracy: 0.1010\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.1714 - accuracy: 0.6165 - val_loss: 2.3637 - val_accuracy: 0.1017\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.1032 - accuracy: 0.6407 - val_loss: 2.3535 - val_accuracy: 0.1022\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 1.0515 - accuracy: 0.6573 - val_loss: 2.3439 - val_accuracy: 0.1034\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 1.0103 - accuracy: 0.6701 - val_loss: 2.3347 - val_accuracy: 0.1053\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.9765 - accuracy: 0.6809 - val_loss: 2.3258 - val_accuracy: 0.1072\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.9480 - accuracy: 0.6893 - val_loss: 2.3171 - val_accuracy: 0.1090\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.9234 - accuracy: 0.6965 - val_loss: 2.3086 - val_accuracy: 0.1105\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.9019 - accuracy: 0.7028 - val_loss: 2.3003 - val_accuracy: 0.1126\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.8829 - accuracy: 0.7086 - val_loss: 2.2921 - val_accuracy: 0.1149\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.8659 - accuracy: 0.7136 - val_loss: 2.2841 - val_accuracy: 0.1166\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.8505 - accuracy: 0.7187 - val_loss: 2.2761 - val_accuracy: 0.1179\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.8365 - accuracy: 0.7235 - val_loss: 2.2683 - val_accuracy: 0.1201\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.8236 - accuracy: 0.7276 - val_loss: 2.2605 - val_accuracy: 0.1221\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.8118 - accuracy: 0.7311 - val_loss: 2.2528 - val_accuracy: 0.1243\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.8008 - accuracy: 0.7342 - val_loss: 2.2451 - val_accuracy: 0.1258\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.7906 - accuracy: 0.7374 - val_loss: 2.2375 - val_accuracy: 0.1281\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.7810 - accuracy: 0.7405 - val_loss: 2.2300 - val_accuracy: 0.1302\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.7721 - accuracy: 0.7434 - val_loss: 2.2225 - val_accuracy: 0.1319\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.7637 - accuracy: 0.7460 - val_loss: 2.2151 - val_accuracy: 0.1340\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.7557 - accuracy: 0.7485 - val_loss: 2.2076 - val_accuracy: 0.1366\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.7482 - accuracy: 0.7506 - val_loss: 2.2003 - val_accuracy: 0.1378\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.7411 - accuracy: 0.7528 - val_loss: 2.1929 - val_accuracy: 0.1407\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.7343 - accuracy: 0.7544 - val_loss: 2.1856 - val_accuracy: 0.1427\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.7279 - accuracy: 0.7562 - val_loss: 2.1783 - val_accuracy: 0.1453\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.7218 - accuracy: 0.7584 - val_loss: 2.1711 - val_accuracy: 0.1464\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.7159 - accuracy: 0.7605 - val_loss: 2.1638 - val_accuracy: 0.1484\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.7103 - accuracy: 0.7623 - val_loss: 2.1566 - val_accuracy: 0.1505\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.7050 - accuracy: 0.7639 - val_loss: 2.1495 - val_accuracy: 0.1519\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6998 - accuracy: 0.7657 - val_loss: 2.1423 - val_accuracy: 0.1539\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.6949 - accuracy: 0.7674 - val_loss: 2.1352 - val_accuracy: 0.1554\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.6901 - accuracy: 0.7693 - val_loss: 2.1280 - val_accuracy: 0.1576\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6856 - accuracy: 0.7709 - val_loss: 2.1209 - val_accuracy: 0.1594\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.6812 - accuracy: 0.7722 - val_loss: 2.1139 - val_accuracy: 0.1611\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6769 - accuracy: 0.7735 - val_loss: 2.1068 - val_accuracy: 0.1637\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6728 - accuracy: 0.7749 - val_loss: 2.0997 - val_accuracy: 0.1658\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6689 - accuracy: 0.7761 - val_loss: 2.0927 - val_accuracy: 0.1680\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6650 - accuracy: 0.7773 - val_loss: 2.0856 - val_accuracy: 0.1706\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.6613 - accuracy: 0.7785 - val_loss: 2.0786 - val_accuracy: 0.1741\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6577 - accuracy: 0.7794 - val_loss: 2.0716 - val_accuracy: 0.1770\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6543 - accuracy: 0.7809 - val_loss: 2.0646 - val_accuracy: 0.1804\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6509 - accuracy: 0.7821 - val_loss: 2.0576 - val_accuracy: 0.1834\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.6476 - accuracy: 0.7831 - val_loss: 2.0506 - val_accuracy: 0.1858\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.6444 - accuracy: 0.7839 - val_loss: 2.0437 - val_accuracy: 0.1897\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6414 - accuracy: 0.7852 - val_loss: 2.0367 - val_accuracy: 0.1944\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6384 - accuracy: 0.7860 - val_loss: 2.0297 - val_accuracy: 0.1993\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.6355 - accuracy: 0.7870 - val_loss: 2.0228 - val_accuracy: 0.2032\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6326 - accuracy: 0.7882 - val_loss: 2.0158 - val_accuracy: 0.2077\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.6298 - accuracy: 0.7889 - val_loss: 2.0088 - val_accuracy: 0.2114\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.6272 - accuracy: 0.7894 - val_loss: 2.0019 - val_accuracy: 0.2151\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6245 - accuracy: 0.7904 - val_loss: 1.9950 - val_accuracy: 0.2190\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.6220 - accuracy: 0.7914 - val_loss: 1.9880 - val_accuracy: 0.2219\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6195 - accuracy: 0.7920 - val_loss: 1.9811 - val_accuracy: 0.2254\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6170 - accuracy: 0.7926 - val_loss: 1.9741 - val_accuracy: 0.2278\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6147 - accuracy: 0.7934 - val_loss: 1.9672 - val_accuracy: 0.2321\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.6124 - accuracy: 0.7943 - val_loss: 1.9602 - val_accuracy: 0.2348\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6101 - accuracy: 0.7947 - val_loss: 1.9533 - val_accuracy: 0.2371\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.6079 - accuracy: 0.7955 - val_loss: 1.9464 - val_accuracy: 0.2408\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.6057 - accuracy: 0.7961 - val_loss: 1.9394 - val_accuracy: 0.2441\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6036 - accuracy: 0.7966 - val_loss: 1.9325 - val_accuracy: 0.2469\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6015 - accuracy: 0.7971 - val_loss: 1.9255 - val_accuracy: 0.2495\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5995 - accuracy: 0.7975 - val_loss: 1.9186 - val_accuracy: 0.2524\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5975 - accuracy: 0.7980 - val_loss: 1.9116 - val_accuracy: 0.2549\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.5956 - accuracy: 0.7984 - val_loss: 1.9047 - val_accuracy: 0.2568\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5937 - accuracy: 0.7988 - val_loss: 1.8977 - val_accuracy: 0.2590\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5918 - accuracy: 0.7994 - val_loss: 1.8908 - val_accuracy: 0.2606\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5900 - accuracy: 0.7998 - val_loss: 1.8838 - val_accuracy: 0.2629\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5882 - accuracy: 0.8003 - val_loss: 1.8769 - val_accuracy: 0.2652\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5864 - accuracy: 0.8010 - val_loss: 1.8699 - val_accuracy: 0.2674\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5847 - accuracy: 0.8016 - val_loss: 1.8629 - val_accuracy: 0.2700\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5830 - accuracy: 0.8020 - val_loss: 1.8559 - val_accuracy: 0.2721\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5813 - accuracy: 0.8024 - val_loss: 1.8490 - val_accuracy: 0.2759\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.5797 - accuracy: 0.8029 - val_loss: 1.8420 - val_accuracy: 0.2809\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.5781 - accuracy: 0.8034 - val_loss: 1.8350 - val_accuracy: 0.2842\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5765 - accuracy: 0.8039 - val_loss: 1.8280 - val_accuracy: 0.2883\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.5750 - accuracy: 0.8045 - val_loss: 1.8210 - val_accuracy: 0.2937\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.5735 - accuracy: 0.8050 - val_loss: 1.8140 - val_accuracy: 0.2995\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5720 - accuracy: 0.8055 - val_loss: 1.8070 - val_accuracy: 0.3060\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.5705 - accuracy: 0.8058 - val_loss: 1.8000 - val_accuracy: 0.3125\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.5691 - accuracy: 0.8063 - val_loss: 1.7930 - val_accuracy: 0.3192\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5677 - accuracy: 0.8068 - val_loss: 1.7859 - val_accuracy: 0.3270\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5663 - accuracy: 0.8073 - val_loss: 1.7789 - val_accuracy: 0.3346\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5649 - accuracy: 0.8077 - val_loss: 1.7719 - val_accuracy: 0.3424\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5636 - accuracy: 0.8080 - val_loss: 1.7648 - val_accuracy: 0.3507\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5623 - accuracy: 0.8086 - val_loss: 1.7578 - val_accuracy: 0.3594\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.5610 - accuracy: 0.8091 - val_loss: 1.7507 - val_accuracy: 0.3680\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.5597 - accuracy: 0.8095 - val_loss: 1.7437 - val_accuracy: 0.3754\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.5584 - accuracy: 0.8099 - val_loss: 1.7366 - val_accuracy: 0.3844\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.5572 - accuracy: 0.8104 - val_loss: 1.7296 - val_accuracy: 0.3936\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5560 - accuracy: 0.8108 - val_loss: 1.7225 - val_accuracy: 0.4020\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5548 - accuracy: 0.8112 - val_loss: 1.7154 - val_accuracy: 0.4109\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5536 - accuracy: 0.8115 - val_loss: 1.7083 - val_accuracy: 0.4198\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.5524 - accuracy: 0.8119 - val_loss: 1.7013 - val_accuracy: 0.4296\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5513 - accuracy: 0.8122 - val_loss: 1.6942 - val_accuracy: 0.4376\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.5501 - accuracy: 0.8128 - val_loss: 1.6871 - val_accuracy: 0.4460\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5490 - accuracy: 0.8131 - val_loss: 1.6800 - val_accuracy: 0.4549\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5479 - accuracy: 0.8134 - val_loss: 1.6729 - val_accuracy: 0.4623\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5468 - accuracy: 0.8137 - val_loss: 1.6658 - val_accuracy: 0.4731\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5458 - accuracy: 0.8141 - val_loss: 1.6586 - val_accuracy: 0.4809\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5447 - accuracy: 0.8145 - val_loss: 1.6515 - val_accuracy: 0.4891\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5437 - accuracy: 0.8150 - val_loss: 1.6444 - val_accuracy: 0.4979\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5426 - accuracy: 0.8153 - val_loss: 1.6373 - val_accuracy: 0.5054\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5416 - accuracy: 0.8156 - val_loss: 1.6302 - val_accuracy: 0.5138\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5406 - accuracy: 0.8161 - val_loss: 1.6230 - val_accuracy: 0.5216\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5396 - accuracy: 0.8164 - val_loss: 1.6159 - val_accuracy: 0.5306\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.5387 - accuracy: 0.8167 - val_loss: 1.6088 - val_accuracy: 0.5401\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5377 - accuracy: 0.8167 - val_loss: 1.6016 - val_accuracy: 0.5472\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5368 - accuracy: 0.8170 - val_loss: 1.5945 - val_accuracy: 0.5544\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5358 - accuracy: 0.8173 - val_loss: 1.5873 - val_accuracy: 0.5627\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.5349 - accuracy: 0.8177 - val_loss: 1.5802 - val_accuracy: 0.5723\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.5340 - accuracy: 0.8180 - val_loss: 1.5730 - val_accuracy: 0.5803\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5331 - accuracy: 0.8183 - val_loss: 1.5659 - val_accuracy: 0.5877\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5322 - accuracy: 0.8185 - val_loss: 1.5588 - val_accuracy: 0.5957\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5313 - accuracy: 0.8189 - val_loss: 1.5516 - val_accuracy: 0.6014\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5304 - accuracy: 0.8190 - val_loss: 1.5445 - val_accuracy: 0.6072\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5296 - accuracy: 0.8192 - val_loss: 1.5373 - val_accuracy: 0.6137\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5287 - accuracy: 0.8195 - val_loss: 1.5302 - val_accuracy: 0.6200\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5279 - accuracy: 0.8197 - val_loss: 1.5230 - val_accuracy: 0.6268\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5271 - accuracy: 0.8199 - val_loss: 1.5159 - val_accuracy: 0.6324\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5263 - accuracy: 0.8202 - val_loss: 1.5087 - val_accuracy: 0.6390\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5255 - accuracy: 0.8205 - val_loss: 1.5016 - val_accuracy: 0.6440\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5247 - accuracy: 0.8208 - val_loss: 1.4944 - val_accuracy: 0.6505\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.5239 - accuracy: 0.8209 - val_loss: 1.4873 - val_accuracy: 0.6555\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5231 - accuracy: 0.8212 - val_loss: 1.4802 - val_accuracy: 0.6596\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.5223 - accuracy: 0.8215 - val_loss: 1.4731 - val_accuracy: 0.6651\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5216 - accuracy: 0.8217 - val_loss: 1.4659 - val_accuracy: 0.6694\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.5208 - accuracy: 0.8219 - val_loss: 1.4588 - val_accuracy: 0.6731\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5201 - accuracy: 0.8221 - val_loss: 1.4517 - val_accuracy: 0.6784\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5193 - accuracy: 0.8224 - val_loss: 1.4446 - val_accuracy: 0.6828\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5186 - accuracy: 0.8226 - val_loss: 1.4375 - val_accuracy: 0.6860\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5179 - accuracy: 0.8229 - val_loss: 1.4304 - val_accuracy: 0.6918\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5171 - accuracy: 0.8230 - val_loss: 1.4233 - val_accuracy: 0.6945\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5164 - accuracy: 0.8232 - val_loss: 1.4162 - val_accuracy: 0.6984\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.5157 - accuracy: 0.8234 - val_loss: 1.4091 - val_accuracy: 0.7010\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5151 - accuracy: 0.8237 - val_loss: 1.4020 - val_accuracy: 0.7048\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.5144 - accuracy: 0.8238 - val_loss: 1.3950 - val_accuracy: 0.7081\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5137 - accuracy: 0.8240 - val_loss: 1.3879 - val_accuracy: 0.7118\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5130 - accuracy: 0.8242 - val_loss: 1.3809 - val_accuracy: 0.7143\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.5124 - accuracy: 0.8243 - val_loss: 1.3738 - val_accuracy: 0.7174\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5117 - accuracy: 0.8245 - val_loss: 1.3668 - val_accuracy: 0.7201\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.5110 - accuracy: 0.8248 - val_loss: 1.3598 - val_accuracy: 0.7227\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5104 - accuracy: 0.8252 - val_loss: 1.3528 - val_accuracy: 0.7248\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5098 - accuracy: 0.8254 - val_loss: 1.3458 - val_accuracy: 0.7265\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.5091 - accuracy: 0.8258 - val_loss: 1.3388 - val_accuracy: 0.7292\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5085 - accuracy: 0.8260 - val_loss: 1.3319 - val_accuracy: 0.7324\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.5079 - accuracy: 0.8262 - val_loss: 1.3249 - val_accuracy: 0.7351\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5073 - accuracy: 0.8264 - val_loss: 1.3180 - val_accuracy: 0.7367\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5067 - accuracy: 0.8266 - val_loss: 1.3111 - val_accuracy: 0.7383\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5061 - accuracy: 0.8268 - val_loss: 1.3041 - val_accuracy: 0.7407\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.5055 - accuracy: 0.8270 - val_loss: 1.2972 - val_accuracy: 0.7426\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5049 - accuracy: 0.8273 - val_loss: 1.2904 - val_accuracy: 0.7450\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.5043 - accuracy: 0.8275 - val_loss: 1.2835 - val_accuracy: 0.7469\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.5037 - accuracy: 0.8277 - val_loss: 1.2767 - val_accuracy: 0.7485\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.5031 - accuracy: 0.8279 - val_loss: 1.2698 - val_accuracy: 0.7499\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5026 - accuracy: 0.8279 - val_loss: 1.2630 - val_accuracy: 0.7516\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5020 - accuracy: 0.8282 - val_loss: 1.2562 - val_accuracy: 0.7528\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.5014 - accuracy: 0.8285 - val_loss: 1.2495 - val_accuracy: 0.7542\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5009 - accuracy: 0.8285 - val_loss: 1.2427 - val_accuracy: 0.7558\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5003 - accuracy: 0.8288 - val_loss: 1.2360 - val_accuracy: 0.7568\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4998 - accuracy: 0.8289 - val_loss: 1.2292 - val_accuracy: 0.7585\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.4992 - accuracy: 0.8291 - val_loss: 1.2226 - val_accuracy: 0.7601\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4987 - accuracy: 0.8292 - val_loss: 1.2159 - val_accuracy: 0.7616\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.4982 - accuracy: 0.8295 - val_loss: 1.2092 - val_accuracy: 0.7624\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.4976 - accuracy: 0.8296 - val_loss: 1.2026 - val_accuracy: 0.7641\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4971 - accuracy: 0.8299 - val_loss: 1.1960 - val_accuracy: 0.7656\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4966 - accuracy: 0.8302 - val_loss: 1.1894 - val_accuracy: 0.7665\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4961 - accuracy: 0.8303 - val_loss: 1.1829 - val_accuracy: 0.7671\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4956 - accuracy: 0.8305 - val_loss: 1.1763 - val_accuracy: 0.7680\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4951 - accuracy: 0.8306 - val_loss: 1.1698 - val_accuracy: 0.7689\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.4946 - accuracy: 0.8307 - val_loss: 1.1633 - val_accuracy: 0.7702\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4941 - accuracy: 0.8309 - val_loss: 1.1569 - val_accuracy: 0.7710\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.4936 - accuracy: 0.8311 - val_loss: 1.1504 - val_accuracy: 0.7726\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.4931 - accuracy: 0.8311 - val_loss: 1.1440 - val_accuracy: 0.7742\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4926 - accuracy: 0.8313 - val_loss: 1.1376 - val_accuracy: 0.7750\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4921 - accuracy: 0.8313 - val_loss: 1.1313 - val_accuracy: 0.7755\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4917 - accuracy: 0.8315 - val_loss: 1.1250 - val_accuracy: 0.7766\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4912 - accuracy: 0.8316 - val_loss: 1.1187 - val_accuracy: 0.7779\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.4907 - accuracy: 0.8317 - val_loss: 1.1124 - val_accuracy: 0.7792\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4902 - accuracy: 0.8318 - val_loss: 1.1062 - val_accuracy: 0.7806\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.4898 - accuracy: 0.8319 - val_loss: 1.1000 - val_accuracy: 0.7814\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4893 - accuracy: 0.8322 - val_loss: 1.0938 - val_accuracy: 0.7826\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.4889 - accuracy: 0.8323 - val_loss: 1.0876 - val_accuracy: 0.7841\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4884 - accuracy: 0.8323 - val_loss: 1.0815 - val_accuracy: 0.7854\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4880 - accuracy: 0.8324 - val_loss: 1.0754 - val_accuracy: 0.7865\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.4875 - accuracy: 0.8325 - val_loss: 1.0694 - val_accuracy: 0.7878\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.4871 - accuracy: 0.8325 - val_loss: 1.0633 - val_accuracy: 0.7882\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.4866 - accuracy: 0.8326 - val_loss: 1.0573 - val_accuracy: 0.7885\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4862 - accuracy: 0.8328 - val_loss: 1.0514 - val_accuracy: 0.7894\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.4858 - accuracy: 0.8329 - val_loss: 1.0455 - val_accuracy: 0.7900\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.4853 - accuracy: 0.8330 - val_loss: 1.0396 - val_accuracy: 0.7909\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4849 - accuracy: 0.8329 - val_loss: 1.0337 - val_accuracy: 0.7914\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.4845 - accuracy: 0.8331 - val_loss: 1.0279 - val_accuracy: 0.7920\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.4841 - accuracy: 0.8332 - val_loss: 1.0221 - val_accuracy: 0.7934\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4836 - accuracy: 0.8334 - val_loss: 1.0163 - val_accuracy: 0.7943\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.4832 - accuracy: 0.8334 - val_loss: 1.0106 - val_accuracy: 0.7959\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.4828 - accuracy: 0.8335 - val_loss: 1.0049 - val_accuracy: 0.7966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1feb47d828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}